# ADR003: Dynamic ETL Using IOC and Futuristic Features

## Status  
Accepted  

## Context  
The **Dynamic ETL** pipeline is designed for flexibility and scalability, enabling the addition of **new data points**, **transformers**, and **notification services** at runtime without predefined configurations. This approach is necessary to accommodate user-defined queries and evolving business requirements, as outlined in the [ETL Specifications Document](https://docs.google.com/document/d/1_rZ8OyYdVETFfojTnDDSAstpRGWeZb_tZMXwQ99BpRQ/edit?tab=t.0).

Key features of the **Dynamic ETL** pipeline include:  
1. **Dynamic Query Construction**: Queries are constructed at runtime based on user requirements (e.g., GraphQL, REST, gRPC).  
2. **Auto-Transformations**: Dynamic transformers are automatically created based on metadata and user-defined rules.  
3. **Modular Notification Services**: Notifications are triggered for data updates, failures, or predefined events.  
4. **Caching and Local Storage**: Data is cached for quick access and stored in local databases for persistent queries.  
5. **Configurable Sources and Loaders**: Sources (e.g., blockchain nodes, APIs) and loaders (e.g., databases) are parameterized and extensible.  
6. **Futuristic Scope**: The pipeline is designed to evolve, leveraging AI/LLMs to parameterize data points, construct queries, and define storage logic automatically.

## Decision  

The Dynamic ETL pipeline MUST:  
1. Follow the **IOC principle** to allow injecting notification services, query constructors, transformers, and storage handlers dynamically.  
2. Construct queries **dynamically** based on user input, supporting REST, GraphQL, gRPC, and other protocols.  
3. Automatically generate and configure transformers for user-defined data points.  
4. Trigger notifications to alert users of updates or events during the ETL process.  
5. Cache results locally to minimize redundant requests and ensure faster response times.  
6. Maintain modularity, enabling components like fetchers, transformers, and storage handlers to be swapped out independently.  

The pipeline will evolve in the long run, incorporating **AI-driven parameterization** to support complex queries, optimized caching strategies, and adaptive storage schemas.

## Implementation Details  

### **Dynamic ETL Design**

```python
class QueryConstructor:
    """Constructs queries dynamically based on user input."""
    def construct(self, source: str, data_point: str, request_type: str = "REST") -> str:
        if request_type == "REST":
            return f"{source}/api/{data_point}"  # Example REST query
        elif request_type == "GraphQL":
            return f"query {{ {data_point} {{ id, value }} }}"  # Example GraphQL query
        elif request_type == "gRPC":
            return f"gRPC method call for {data_point}"  # Example gRPC request
        else:
            raise ValueError(f"Unsupported request type: {request_type}")


class DynamicTransformer:
    """Dynamically creates transformers based on metadata and user rules."""
    def transform(self, data_point: str, raw_data: dict, rules: dict = None) -> dict:
        # Apply user-defined transformation rules
        if not rules:
            rules = {"filter": lambda x: x}  # Default: No transformation
        return {key: rules.get("filter", lambda x: x)(value) for key, value in raw_data.items()}


class NotificationService:
    """Handles notifications for ETL pipeline events."""
    def notify(self, event: str, data_point: str):
        print(f"Notification: {event} triggered for data point {data_point}")


class DynamicETL:
    """Dynamic ETL pipeline with IOC-based modularity."""
    def __init__(self, query_constructor, transformer, storage, notifier, cache):
        self.query_constructor = query_constructor
        self.transformer = transformer
        self.storage = storage
        self.notifier = notifier
        self.cache = cache

    def execute_pipeline(self, data_points: list, source: str, request_type: str = "REST"):
        """Execute the pipeline dynamically for given data points."""
        for data_point in data_points:
            query = self.query_constructor.construct(source, data_point, request_type)

            # Check cache for existing data
            if cached_data := self.cache.get(data_point):
                print(f"Cache hit for {data_point}")
                transformed_data = cached_data
            else:
                print(f"Fetching data for {data_point} using {query}")
                raw_data = self.fetch_data(query)
                transformed_data = self.transformer.transform(data_point, raw_data)
                self.cache.store(data_point, transformed_data)

            self.storage.store(data_point, transformed_data)
            self.notifier.notify("Data Updated", data_point)

    def fetch_data(self, query: str) -> dict:
        """Simulate data fetching."""
        # Simulated raw data
        return {"id": 1, "value": 100}  # Example data


# Example Usage
class MockCache:
    """Mock cache implementation."""
    def __init__(self):
        self.store = {}

    def get(self, key):
        return self.store.get(key)

    def store(self, key, value):
        self.store[key] = value


class MockStorage:
    """Mock storage implementation."""
    def store(self, data_point, data):
        print(f"Stored {data_point}: {data}")


if __name__ == "__main__":
    # Instantiate components
    query_constructor = QueryConstructor()
    transformer = DynamicTransformer()
    storage = MockStorage()
    notifier = NotificationService()
    cache = MockCache()

    # Dynamic ETL pipeline
    etl_pipeline = DynamicETL(query_constructor, transformer, storage, notifier, cache)

    # Execute pipeline dynamically
    etl_pipeline.execute_pipeline(["token_volume", "wallet_activity"], source="https://api.blockchain.com", request_type="REST")
