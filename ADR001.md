# ADR001: Modular Design Using IoC

## Status  
Accepted ?? 

## Context  
The ETL pipeline must support both **static ETL** and **dynamic ETL**, ensuring modularity for evolving requirements. Using **Inversion of Control (IoC)** guarantees reusability, testability, and ease of integration for new functionalities without tightly coupling components.

## Decision  
The pipeline MUST adhere to the **IoC design principle** to achieve modularity. Each major component (source handlers, transformers, and storage handlers) MUST implement well-defined interfaces and SHOULD NOT directly depend on each other. Dependency injection MUST be used to allow the pipeline to operate in a plug-and-play manner.

## Why IoC?  
- **Reusability**: Components such as data source handlers and transformers can be swapped without modifying the pipeline.  
- **Testability**: Mock dependencies can easily replace real components in test environments.  
- **Extensibility**: Adding new sources, transformers, or storage options does not require changes to the core pipeline logic.  

## Code Snippet  
Below is an example of IoC applied to an ETL pipeline:

```python
# Interfaces for IoC
from typing import Protocol, Any

class SourceHandler(Protocol):
    def fetch_data(self, query: str) -> Any:
        ...

class Transformer(Protocol):
    def transform(self, raw_data: Any) -> Any:
        ...

class StorageHandler(Protocol):
    def store(self, transformed_data: Any) -> None:
        ...

# Concrete implementations
class APIHandler:
    def fetch_data(self, query: str) -> dict:
        # Simulated data fetch
        return {"raw_data": "sample_data"}

class BasicTransformer:
    def transform(self, raw_data: dict) -> dict:
        return {"transformed_data": raw_data["raw_data"].upper()}

class LocalStorage:
    def store(self, transformed_data: dict) -> None:
        print(f"Stored: {transformed_data}")

# ETL Pipeline
class ETLPipeline:
    def __init__(self, source: SourceHandler, transformer: Transformer, storage: StorageHandler):
        self.source = source
        self.transformer = transformer
        self.storage = storage

    def execute(self, query: str):
        raw_data = self.source.fetch_data(query)
        transformed_data = self.transformer.transform(raw_data)
        self.storage.store(transformed_data)

# Main setup
if __name__ == "__main__":
    source = APIHandler()
    transformer = BasicTransformer()
    storage = LocalStorage()

    pipeline = ETLPipeline(source, transformer, storage)
    pipeline.execute(query="Fetch token data")
